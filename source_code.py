# -*- coding: utf-8 -*-
"""nitin_finalNLP.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11DgS5C-IL30clHzhGjFg3H2W_Ya_CCIE
"""

# !pip install num2words

# from google.colab import drive
# drive.mount('/content/gdrive')

import string
from curses.ascii import isdigit
from turtle import forward
from unittest import expectedFailure
from num2words import num2words
import torch
from torch import nn
from torch.utils.data import DataLoader, Dataset
import pandas
from torch import optim
from torch.autograd import Variable
import numpy as np
# from source_code import oneValued_data

# gdrive_path = '/content/gdrive/My Drive/'
noOfTrainLines = 20000
noOfTestLines = 30000
# file_open = open(f'{gdrive_path}corpous.txt','r+')
file_open = open('corpous.txt','r+')

learning_rate = 1e-3
# learning_rate = 1e-2

# batch_size = 6000

courpous_words = []
translate = str.maketrans('', '', string.punctuation)
lineFromFile = file_open.readlines()[0:noOfTrainLines]
file_open.close()
file_open = open('corpous.txt','r+')
lineFromFile2 = file_open.readlines()[noOfTrainLines:noOfTestLines]
file_open.close()
print(len(lineFromFile2))
for i in range(0,noOfTrainLines):
    lineFromFile[i] = lineFromFile[i].lower()
    lineFromFile[i] = lineFromFile[i].translate(translate)
    lineFromFile[i] = lineFromFile[i].split()
    for j in range(0,len(lineFromFile[i])):
        if(lineFromFile[i][j].isdigit()):
            lineFromFile[i][j] = num2words(lineFromFile[i][j])
            lineFromFile[i][j] = lineFromFile[i][j].translate(translate)
    courpous_words += lineFromFile[i]

courpous_wordsTest = []
for i in range(0,noOfTestLines-noOfTrainLines-1):
    lineFromFile2[i] = lineFromFile2[i].lower()
    lineFromFile2[i] = lineFromFile2[i].translate(translate)
    lineFromFile2[i] = lineFromFile2[i].split()
    for j in range(0,len(lineFromFile2[i])):
        if(lineFromFile2[i][j].isdigit()):
            lineFromFile2[i][j] = num2words(lineFromFile2[i][j])
            lineFromFile2[i][j] = lineFromFile2[i][j].translate(translate)
    courpous_wordsTest += lineFromFile2[i]

dim_file = 50
n_gram = 4
# file_open1 = open(f'{gdrive_path}glove.6B.50d.txt','r+')
file_open1 = open('glove.6B.50d.txt','r+')
wordOfBag = []
lineFromBag = file_open1.readlines()
for i in range(0,len(lineFromBag)):
    lineFromBag[i] = lineFromBag[i].split()
    wordOfBag.append(lineFromBag[i][0])
    del lineFromBag[i][0]

# print(lineFromBag[4])

# device = "cuda" if torch.cuda.is_available() else "cpu"
# print(f"Using {device} device")

class NeuralNetwork(nn.Module):
    def __init__(self):
        super(NeuralNetwork, self).__init__()
        self.flatten = nn.Flatten()
    
        self.linear_relu_stack = nn.Sequential(
            nn.Linear(200, 300),
            nn.ReLU(),
            nn.Linear(300, 300),
            nn.ReLU(),
            nn.Linear(300,50),
        )

    def forward(self, x):
        x = self.flatten(x)
        logits = self.linear_relu_stack(x)
        pred_probab = nn.Softmax(dim=1)(logits)
        return pred_probab



def GetTensorOfOneWord(word,wordOfBag,lineFromBag):
    index = wordOfBag.index(word)
    floatingList = lineFromBag[index]
    for i in range (0,dim_file):
        floatingList[i] = float(floatingList[i])
    floatingTensor = torch.FloatTensor(floatingList)
    floatingTensor = floatingTensor.view(1,dim_file)
    return floatingTensor

ngram = [((courpous_words[i],courpous_words[i+1],courpous_words[i+2],courpous_words[i+3]),courpous_words[i+4])
        for i in range(0,len(courpous_words)-4)] 
# print(len(courpous_words))
# print((ngram))
ngramTest = [((courpous_wordsTest[i],courpous_wordsTest[i+1],courpous_wordsTest[i+2],courpous_wordsTest[i+3]),courpous_wordsTest[i+4])
        for i in range(0,len(courpous_wordsTest)-4)] 

    
# print(ngram[0][0])
def ngram_tensor(ngram,wordOfBag,lineFromBag,startOfIndex):
    floatingList = []
    index1 = wordOfBag.index(ngram[startOfIndex][0][0])
    index2 = wordOfBag.index(ngram[startOfIndex][0][1])
    index3 = wordOfBag.index(ngram[startOfIndex][0][2])
    index4 = wordOfBag.index(ngram[startOfIndex][0][3])
    floatingList += lineFromBag[index1]
    floatingList += (lineFromBag[index2])
    floatingList += (lineFromBag[index3])
    floatingList += (lineFromBag[index4])
    for i in range (0,200):
        floatingList[i] = float(floatingList[i])
    floatingTensor = torch.FloatTensor(floatingList)
    floatingTensor = floatingTensor.view(1,200)
    return floatingTensor

model = NeuralNetwork()#.to(device)
print(model)



optimizer = torch.optim.SGD(model.parameters(), lr=learning_rate)
# optimizer = optim.Adam([p for p in model.parameters() if p.requires_grad], lr=0.001)
lossFun = nn.CrossEntropyLoss()
optimizer.zero_grad()
def train_data(ngram,wordOfBag,lineFromBag):
    loss_count = 0
    j=0
    fileToWrite = open('2020102046-LM1-perplexity.txt','w+')
    # optimizer.zero_grad()
    for i in range(0,len(ngram)):
        if((ngram[i][0][0] in wordOfBag) & (ngram[i][0][1] in wordOfBag) & (ngram[i][0][2] in wordOfBag) & (ngram[i][0][3] in wordOfBag)):    
            inputVector = ngram_tensor(ngram,wordOfBag,lineFromBag,i)
            outputVector = model(inputVector)
            # y_pred = outputVector.argmax(1)
            # OutputWord = courpous_words[y_pred]
            # if(OutputWord in wordOfBag):
            # Output = GetTensorOfOneWord(OutputWord,wordOfBag,lineFromBag)
            if(ngram[i][1] in wordOfBag):
              PredictedOutput = GetTensorOfOneWord(ngram[i][1],wordOfBag,lineFromBag)
              PredictedOutput.requires_grad = True
              loss = lossFun(outputVector[0],PredictedOutput[0])
              perplexity_score = np.exp(loss.item())
              perplexity_score = "The score for the " + str(j)+ "th ngram is "+ str(perplexity_score) + "\n" 
              fileToWrite.write(perplexity_score)
            #   print(perplexity_score)
              loss.backward()
              optimizer.step()
              loss_count = loss_count - loss
              j += 1
           
        if(i % 10000 == 0):
            loss_avg = loss_count / j 
            print(f"loss: {loss_avg:>7f}  [{i:>5d}/{len(courpous_words):>5d}]")

def test_data(ngram,wordOfBag,lineFromBag):
    predict_count = 0
    j=0
    for i in range(0,len(ngram)):
        if((ngram[i][0][0] in wordOfBag) & (ngram[i][0][1] in wordOfBag) & (ngram[i][0][2] in wordOfBag) & (ngram[i][0][3] in wordOfBag)):    
            inputVector = ngram_tensor(ngram,wordOfBag,lineFromBag,i)
            outputVector = model(inputVector)
            y_pred = outputVector.argmax(1)
            OutputWord = courpous_words[y_pred]
            if(OutputWord in wordOfBag):
              Output = GetTensorOfOneWord(OutputWord,wordOfBag,lineFromBag)
              if(ngram[i][1] in wordOfBag):
                PredictedOutput = GetTensorOfOneWord(ngram[i][1],wordOfBag,lineFromBag)
                # PredictedOutput.requires_grad = True
                # optimizer.zero_grad()
                # loss = lossFun(outputVector[0],PredictedOutput[0])
                # loss.backward()
                # optimizer.step()
                if(OutputWord == ngram[i][1]):
                  predict_count += 1
                j += 1
           
        if(i % 10000 == 0):
            loss_avg = predict_count / j 
            print(f"correctness: {loss_avg*100:>7f}%  [{i:>5d}/{len(courpous_words):>5d}]")

epoch = 1
for epoch_counter in range(0,epoch):
    print(f"Epoch {epoch_counter+1}\n-------------------------------")
    train_data(ngram,wordOfBag,lineFromBag)
    test_data(ngramTest,wordOfBag,lineFromBag)
print("Done!")

torch.save(model.state_dict(), 'model_weights.pth')
